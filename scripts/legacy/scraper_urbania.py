# scraper_urbania_stealth.py
import time
import pandas as pd
from selenium import webdriver
from selenium_stealth import stealth
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import re
import os

class UrbaniaStealthScraper:
    def __init__(self, headless=False):
        print("üïµÔ∏è  INICIANDO SCRAPER STEALTH (Anti-detection)")
        
        # 1. CONFIGURACI√ìN AVANZADA DE CHROME
        options = webdriver.ChromeOptions()
        
        if not headless:  # SIEMPRE visible para debugging
            options.add_argument("start-maximized")
        
        # Opciones cr√≠ticas para evitar detecci√≥n
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        # M√°s opciones para parecer humano
        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-web-security')
        options.add_argument('--allow-running-insecure-content')
        
        # User-Agent personalizado (Windows 10, Chrome real)
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        # 2. INICIAR DRIVER
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=options)
        
        # 3. APLICAR STEALTH (ANTES DE CUALQUIER NAVEGACI√ìN)
        stealth(self.driver,
                languages=["es-ES", "es", "en-US", "en"],
                vendor="Google Inc.",
                platform="Win32",
                webgl_vendor="Intel Inc.",
                renderer="Intel Iris OpenGL Engine",
                fix_hairline=True,
                )
        
        self.wait = WebDriverWait(self.driver, 20)  # Wait m√°s largo
        self.properties_data = []
        
    def navigate_safely(self, url):
        """Navegaci√≥n segura con m√∫ltiples verificaciones"""
        print(f"\nüåê Navegando a: {url}")
        
        try:
            self.driver.get(url)
            
            # Espera inicial CR√çTICA
            time.sleep(5)
            
            # DEBUG: Guardar captura
            self.driver.save_screenshot("debug_urbania_loaded.png")
            print("üì∏ Captura guardada: debug_urbania_loaded.png")
            
            # Verificar si carg√≥ contenido real
            page_source = self.driver.page_source.lower()
            
            # Se√±ales de bloqueo
            block_signals = ["access denied", "cloudflare", "captcha", "robot", "verification"]
            for signal in block_signals:
                if signal in page_source:
                    print(f"‚ö†Ô∏è  POSIBLE BLOQUEO detectado: '{signal}'")
                    return False
            
            # Se√±ales de √©xito
            success_signals = ["departamento", "alquiler", "propiedad", "inmueble", "precio"]
            success_count = sum(1 for signal in success_signals if signal in page_source)
            
            if success_count >= 2:
                print(f"‚úÖ P√°gina cargada correctamente ({success_count} se√±ales de √©xito)")
                return True
            else:
                print("‚ùå P√°gina carg√≥ pero sin contenido esperado")
                return False
                
        except Exception as e:
            print(f"‚ùå Error navegando: {e}")
            return False
    
    def extract_with_patience(self, district):
        """Extrae propiedades con mucha paciencia"""
        print(f"\nüîç BUSCANDO EN: {district.upper()}")
        
        # URL espec√≠fica (probemos diferentes formatos)
        url_variants = [
            f"https://urbania.pe/alquiler-de-departamentos-en-{district}",
            f"https://urbania.pe/buscar/alquiler-departamentos?districts={district}",
            f"https://urbania.pe/buscar/alquiler?districts={district}"
        ]
        
        for url in url_variants:
            print(f"  Probando URL: {url}")
            if self.navigate_safely(url):
                break
            time.sleep(3)
        
        # ESPERA INTELIGENTE para contenido din√°mico
        print("‚è≥ Esperando contenido din√°mico...")
        
        # Intentar diferentes selectores
        selectors_to_try = [
            "div[class*='posting']",
            "article[data-qa*='posting']", 
            "div[data-testid*='posting']",
            ".posting-card",
            ".property-card",
            "div.card"
        ]
        
        for selector in selectors_to_try:
            try:
                print(f"  Buscando con selector: {selector}")
                elements = WebDriverWait(self.driver, 10).until(
                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, selector))
                )
                
                if elements and len(elements) > 3:
                    print(f"‚úÖ Encontrados {len(elements)} elementos con: {selector}")
                    return self.process_elements(elements, district)
                    
            except Exception as e:
                print(f"  Selector fall√≥: {selector}")
                continue
        
        print("‚ùå No se encontraron propiedades con ning√∫n selector")
        return 0
    
    def process_elements(self, elements, district):
        """Procesa los elementos encontrados"""
        print(f"üìä Procesando {len(elements)} elementos...")
        
        count = 0
        for i, element in enumerate(elements[:30]):  # Limitar a 30
            try:
                # Scroll suave a cada elemento
                self.driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", element)
                time.sleep(0.1)
                
                data = self.extract_data_smart(element, district)
                if data:
                    self.properties_data.append(data)
                    count += 1
                    
                    if count % 5 == 0:
                        print(f"  ‚úÖ {count} propiedades extra√≠das")
                        
            except Exception as e:
                if i < 5:  # Solo mostrar primeros errores
                    print(f"  ‚ö†Ô∏è  Error elemento {i}: {str(e)[:40]}")
        
        return count
    
    def extract_data_smart(self, element, district):
        """Extrae datos de forma inteligente"""
        try:
            text = element.text
            
            # Extraer precio (con m√∫ltiples patrones)
            price = 0
            price_match = re.search(r'S/\s*([\d,]+)', text)
            if price_match:
                try:
                    price = float(price_match.group(1).replace(',', ''))
                except:
                    pass
            
            # Solo si tiene precio realista
            if not (500 < price < 50000):
                return None
            
            # Extraer √°rea
            area = 0
            area_match = re.search(r'(\d+)\s*(?:m¬≤|m2|m\s*¬≤)', text)
            if area_match:
                area = float(area_match.group(1))
            
            # Extraer habitaciones
            bedrooms = 0
            bed_match = re.search(r'(\d+)\s*(?:dorm|hab)', text, re.IGNORECASE)
            if bed_match:
                bedrooms = int(bed_match.group(1))
            
            # T√≠tulo (primera l√≠nea no vac√≠a)
            lines = [line.strip() for line in text.split('\n') if line.strip()]
            title = lines[0][:80] if lines else f"Propiedad en {district}"
            
            return {
                'title': title,
                'price': price,
                'area': area,
                'bedrooms': bedrooms,
                'district': district.capitalize(),
                'scraped_date': pd.Timestamp.now().strftime('%Y-%m-%d'),
                'source': 'urbania_stealth'
            }
            
        except Exception as e:
            return None
    
    def save_results(self):
        """Guarda los resultados"""
        if not self.properties_data:
            print("\n‚ùå No hay datos para guardar")
            return None
        
        df = pd.DataFrame(self.properties_data)
        
        # Filtrar
        df = df[df['price'] > 0]
        
        os.makedirs('data/raw', exist_ok=True)
        filename = f"data/raw/urbania_stealth_{pd.Timestamp.now().strftime('%Y%m%d')}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"\nüíæ RESULTADOS:")
        print(f"  Archivo: {filename}")
        print(f"  Propiedades: {len(df)}")
        
        if not df.empty:
            print(f"  Precio promedio: S/ {df['price'].mean():,.0f}")
            print(f"  Distritos √∫nicos: {', '.join(df['district'].unique())}")
        
        return df
    
    def close(self):
        self.driver.quit()
        print("\nüëã Navegador cerrado")

def main():
    print("="*70)
    print("üè† URBANIA SCRAPER CON STEALTH TECHNOLOGY")
    print("="*70)
    print("‚ö†Ô∏è  ADVERTENCIA: Este script intenta evitar sistemas anti-bot")
    print("   Urbania puede bloquear IPs con uso intensivo")
    print("   Usa con moderaci√≥n y fines educativos")
    print("="*70)
    
    scraper = UrbaniaStealthScraper(headless=False)
    
    try:
        # SOLO 2 distritos para prueba
        districts = ['miraflores', 'surco']
        
        total = 0
        for district in districts:
            count = scraper.extract_with_patience(district)
            total += count
            
            if count == 0:
                print(f"‚ö†Ô∏è  No se pudo extraer de {district}, saltando...")
            
            # Pausa larga entre distritos
            if district != districts[-1]:
                print(f"\n‚è≥ Pausa de 10 segundos...")
                time.sleep(10)
        
        print(f"\n{'='*70}")
        print(f"üìä TOTAL: {total} propiedades extra√≠das")
        
        if total > 0:
            df = scraper.save_results()
            if df is not None:
                print(f"\nüéØ MUESTRA DE DATOS:")
                print(df[['district', 'price', 'area', 'bedrooms']].head().to_string())
        else:
            print("‚ùå No se extrajo ninguna propiedad")
            print("\nüí° RECOMENDACI√ìN: Usa datos de muestra y contin√∫a con las APIs")
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Interrumpido por usuario")
    except Exception as e:
        print(f"\n‚ùå Error cr√≠tico: {e}")
    finally:
        scraper.close()
    
    return total > 0

if __name__ == "__main__":
    success = main()
    
    if not success:
        print("\n" + "="*70)
        print("üî• ALTERNATIVA R√ÅPIDA:")
        print("="*70)
        print("Si Urbania bloquea persistentemente, te recomiendo:")
        print("1. Usar datos de muestra realistas que ya tenemos")
        print("2. Continuar con APIs de Google Maps y dashboard")
        print("3. El scraper puede ser proyecto aparte")
        print("\n¬øQuieres que te ayude con los datos de muestra?")